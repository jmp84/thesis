\chapter{String Regeneration as Phrase-Based Translation}
\label{chap:gyro}

%TODO normalize ngram n-gram $n$-gram etc.
%TODO shorter name for the chapter
%TODO make subsubsection numbered
%TODO discuss look ahead

Machine translation output was originally evaluated in terms
of \emph{adequacy}, or how well the meaning of the source
text is preserved in the translated text, and \emph{fluency},
or how well formed is the
translation~\citep{white-oconnell-carlson:1993:HLT}.
There is increasing concern that translation output
often lacks fluency~\citep{knight:2007:TALK}.
In this chapter, we study the related problem of
string regeneration: given a sentence where the
word order has been scrambled, how difficult is it
to recover the original sentence ? Our approach
is inspired by phrase-based translation techniques and
achieves state-of-the-art performance on the string regeneration
task.

% TODO: in section blabla

\section{Introduction}

Before the widespread use of automatic metrics such as
BLEU~\citep{papineni-roukos-ward-zhu:2002:ACL} or
METEOR~\citep{banerjee-lavie:2005:MTSumm}, machine translation
systems were originally evaluated in terms of \emph{adequacy}
and \emph{fluency}~\citep{white-oconnell-carlson:1993:HLT}.
\emph{Adequacy} measures how well the meaning of the
source text is preserved in the translation output while \emph{fluency}
measures how well formed is the translation output.

There is increasing concern that translation output
often lacks fluency~\citep{knight:2007:TALK}.
Current popular automatic metrics for translation can be gamed in the
sense that it is possible to obtain a high score according to these
metric with an output that lacks fluency.
One reason is that these
metrics measure local matches between translation output and
references without ensuring that the overall structure of the output
is sound. This may have led research on translation to neglect fluency.

Even so, obtaining a fluent translation output is difficult.
One of the reasons for lack of fluency is word ordering.
Word ordering is a fundamental problem in many natural language
processing applications.
In machine translation, because
source language and target language have a different word
ordering, a machine translation system needs to model word
reordering. In some language pairs such as French-English, the
source language gives good cues to the target word order, however,
in other language pairs such as Chinese-English or Japanese-English,
the word order can be very different in the source and
the target language. In natural language generation tasks, such
as paraphrasing or summarisation, word ordering
is also critical at the surface realisation
step~\citep{reiter-dale:1997:JNLE}.
Word ordering is also one possible type of grammatical
error, especially for foreign language
learners~\citep{yu-chen:2012:COLING}.

In this chapter, we address the word ordering problem in
isolation through the task of
string regeneration~\citep{wan-dras-dale-paris:2009:EACL}.
Given an input sentence where the word order has
been scrambled, the string regeneration task is to recover
the original sentence. This task can be seen as one of the
steps in surface realisation for language generation where
the input is a list of content words and function words and
the output a grammatical sentence. It also allows us to study
the realization task independently of the translation task.

We use a simple but extensible approach, inspired from
stack-based decoding for phrase-based translation.
Given a bag of word as input, we use $n$-gram rules
extracted from a large monolingual corpus and concatenate
these $n$-grams to form hypotheses. The hypotheses are scored
with a linear model in the same manner as translation
hypotheses in phrase-based translation~\citep{och-ney:2002:ACL}.
One of the critical features in this system
is the language model. Arbitrary features can be added to the
system, demonstrating its flexibility.
% TODO
%In addition, we present
%an alternative version that incorporates a dependency language
%model~\citep{shen-xu-weischedel:2008:ACL,shen-xu-weischedel:2010:CL}.

Our strategy achieves state-of-the-art in the string regeneration
task. We also study various capabilities of our decoder, such as
splitting the input into chunks, pruning, length of rules, rule overlap
and rescoring. % TODO expand on this
% TODO read gyro paper eacl

\section{Background}

\citet{brown-cocke-dellapietra-dellapietra-jelinek-lafferty-mercer-roossin:1990:CL}
introduce the string regeneration task, or \emph{bag of word translation}
to demonstrate the effectiveness
of $n$-gram language models. With a 3-gram language model, they recover
63\% of sentences with less than 11 words with brute force search
over all permutations. This is of course impractical for longer
sentences. For comparison, our system is able to recover
60\% of the tokenized and lowercased sentences with less than 11 words
on Section 23 of the Penn Treebank.
%TODO compare our system to this performance
%TODO update this performance with a better system or with rescored system
%TODO maybe: run regeneration with base noun phrase together
%TODO run gyro with unigrams only for the baseline and try to beat it

String regeneration has recently gained interest as
a standalone
task~\citep{wan-dras-dale-paris:2009:EACL,he-wang-guo-liu:2009:ACLIJCNLP,zhang-clark:2011:EMNLP,zhang-blackwood-clark:2012:EACL2012,zhang:2013:IJCAI}.
%
%wan et al:
%find the best dep tree
%related work: statistical surface realisation Langkilde and Knight
%text to text generation TODO check for papers with that title
%use string regeneration as surrogate for grammaticality test
%content selection is abstracted out
%training on the ptb
%base noun phrase are kept together !!!!!!!!
%string regeneration: generation component of MT, paraphrase generation, summarisation
%debunk claim that lm performs bad: try to get score with unigrams only
%use chu liu edmond algo: get a dep tree.
%now need to get right ordering for example for all left children
%use language model. this is an example of tree linearization
%
%
String regeneration serves as a proxy to measure the grammaticality of a
natural language generation system output and is a simplified version
of the \emph{linguistic realisation} task in natural language
generation~\citep{reiter-dale:1997:JNLE}.
\citet{wan-dras-dale-paris:2009:EACL} use the minimum spanning tree algorithm
to find an optimal tree given a bag of words.
\citet{he-wang-guo-liu:2009:ACLIJCNLP} use a log-linear model to map
dependency relations into surface strings. They achieve a BLEU
score of 88.74\% using the Chinese Dependency Treebank.
CCG grammars~\citep{zhang-clark:2011:EMNLP} have also been used
for the string realisation task. These grammars have also
been combined with large language
models~\citep{zhang-blackwood-clark:2012:EACL2012}.
Dependency-based language model have also been used for the
string realisation task~\citep{guo-wang-vanGenabith:2011:JNLE}.
The works described so far are instances of \emph{tree linearization}: given
a set of unordered dependency, reconstruct a dependency tree which gives
the word ordering~\citep{belz-bohnet-mille-wanner-white:2012:INLG}.
\citet{zhang:2013:IJCAI} extends this task to partial tree linearization: the
input is a set of possibly missing POS tags and possibly missing unordered
dependencies. This last work achieves the best performance on string regeneration
on the Penn Tree Bank so far.

In this chapter, we restrict the input to be a bag of words without any additional
information. To our knowledge, our method achieves state-of-the-art performance
on this task.

% TODO MAYBE THIS
% Towards Developing Generation Algorithms for Text-to-Text Applications
% TODO also review Adria's paper

\section{Phrase-Based Translation Model for String Regeneration}

In the previous section, we have reviewed various strategies for string
regeneration. We now describe our original method, which is inspired from
phrase-based translation techniques~\citep{koehn:2010:book}. As described
in (TODO refer to background), we employ a feature based linear model as our
objective function:
%
\begin{equation}
  \hat{\bm{e}} = \argmax_{\bm{e}} \bm{\lambda} \cdot \bm{\Phi(\bm{e})}
  \label{eq:gyroModel}
\end{equation}
%
where:
%
\begin{itemize}
  \item $\hat{\bm{e}}$ is the best possible hypothesis according to the model.
  \item $\bm{e}$ is a possible hypothesis.
  \item $\bm{\lambda}$ is a feature weight vector.
  \item $\bm{\Phi}(\bm{e})$ is a feature vector for the hypothesis $\bm{e}$.
\end{itemize}
%
$\bm{\Phi}(\bm{e})$ contains a language model
$g(\bm{e})$ and local features $\bm{\varphi}(\bm{e})$ (see TODO refer
to background). Similarly, $\bm{\lambda}$ has a weight $\lambda_g$ for the language model
and weights $\bm{\lambda_{\varphi}}$ for local features. As in phrase-based
translation, a hypothesis
is decomposed into phrases $\bm{e}_1^I$. We can then rewrite
\autoref{eq:gyroModel} into \autoref{eq:gyroModel2}:
%
\begin{equation}
  \begin{split}
    \hat{\bm{e}} &= \argmax_{\bm{e} = \bm{e}_1^I} \lambda_{g} g(\bm{e}) + \bm{\lambda_{\varphi}} \cdot \bm{\varphi}(\bm{e}) \\
                 &= \argmax_{\bm{e} = \bm{e}_1^I} \lambda_{g} g(\bm{e}) + \bm{\lambda_{\varphi}} \cdot \sum_{i = 1}^I \bm{\varphi}(\bm{e}_i) \\
  \end{split}
  \label{eq:gyroModel2}
\end{equation}
%
In this chapter, we will only experiment with the language model feature.
%TODO maybe add experiments with additional features
Our decoder algorithm described subsequently in
\autoref{sec:gyroDecoderAlgorithm} generates hypotheses left to right
by concatenating phrases $e_1$, ..., $e_I$. For a phrase index $j \in [1, I]$,
we define the partial model score $\text{sc}(e_1^j)$ of partial hypothesis $e_1^j$ as:
%
\begin{equation}
  \text{sc}(e_1^j) = \lambda_g g(e_1^j) + \bm{\lambda_{\varphi}} \cdot \sum_{i = 1}^j \bm{\varphi}(\bm{e}_i) \\
\end{equation}

\section{Phrase-Based Translation Rules for String Regeneration}

In the previous section, we introduced our model for string regeneration.
We assumed that a hypothesis could be segmented into phrases. We will
now describe how to obtain these phrases.

\subsection{Rule Extraction}

We first compile a large monolingual corpus. Text is tokenized
and lowercased. We then employ the MapReduce
framework (see TODO refer to background) in order to extract
$n$-grams together with their occurrence counts in the corpus.

The input key to the \emph{map} function is irrelevant.
The input value to the \emph{map} function is a line of text.
The \emph{map} function simply extract all $n$-grams from the
input line of text and outputs these $n$-grams with a count of
one. The input to the \emph{reduce} function is an $n$-gram
and a series of constant values equal to one.
The \emph{reduce} function simply sums the counts of one
and output the $n$-gram with its total count. This MapReduce
job is analogous to the canonical word count example except
that here $n$-grams of higher order than unigrams are considered.
For experiments, we extract $n$-grams from order 1 to 5.

\subsection{Rule Filtering}
\label{sec:ngramRuleFiltering}

Once we have extracted all $n$-grams from a monolingual corpus, we
would like to retrieve the $n$-grams relevant to a test set
that we wish to experiment on.

Contrary to the process described in \autoref{sec:rulextract}, we
do not generate queries and then seek those queries in an HFile.
This is because the number of possible queries is too large
to fit in memory. For a sentence of length $N$ with words distinct
from each other, the number of possible relevant 5-grams is
$N \times (N - 1) \times (N - 2) \times (N - 3) \times (N - 4)$.
For sentence of length 100, this would correspond to approximately
9B queries.

We take the alternative approach of scanning the set of $n$-grams
and retaining the relevant ones. For this, we again use a MapReduce
job. The input key to the \emph{map} function is an $n$-gram and
the input value is a count. For each test sentence, the \emph{map}
function checks if the $n$-gram vocabulary is included in the
vocabulary of that sentence and if so, it generates all possible
coverages of this $n$-gram for that sentence, where a coverage
indicates the positions of the words covered by the $n$-gram
in the input sentence. The \emph{map}
output key is the $n$-gram together with a sentence id. The \emph{map}
output value is a coverage and a count. The \emph{reduce} function
simply removes duplicates in the list of coverages. % TODO explain why
%TODO explain coverage.

\section{Decoder Algorithm}
\label{sec:gyroDecoderAlgorithm}

In this section, we present our NgramGen decoder, which reorders
an input bag of word using techniques inspired from phrase-based
SMT~\citep{koehn:2010:book}, and which we review in (TODO refer to
background).

\subsection{Input}

The input to the decoder consists of:
%
\begin{itemize}
  \item A bag of words or multiset $\bm{w}$, represented as a vector.
    For example, we could have $\bm{w} = [\text{do, you, do}]$
  \item A set of $n$-grams, where the
    vocabulary of each $n$-gram is a subset of the vocabulary of the
    input bag of words. Each $n$-gram is also associated with
    one or more coverages as computed in \autoref{sec:ngramRuleFiltering}.
    For a given $n$-gram, its coverage is a bit vector
    that indicates what positions the $n$-gram covers in the input bag of words.
    In case of repeated words, an $n$-gram can have several possible
    coverages. For example, with the input bag of words
    $[\text{do, you, do}]$, the $n$-gram $[\text{do, you}]$ has
    two possible coverages: 110 and 011.
    The input $n$-grams and coverages are represented as
    an associative array datastructure. We denote this
    input by $\mathcal{N}$. Note that it would be possible for the decoder
    to compute possible coverages on the fly, however we choose to precompute those
    at the filtering step described in \autoref{sec:ngramRuleFiltering}.
  \item A $n$-gram language model $\mathcal{G}$. The $n$-gram
    language model is used as a feature in the model to
    encourage fluent output.
\end{itemize}
%
We will now describe how hypotheses are generated by the decoder
from the input.

\subsection{Algorithm}

Our algorithm is inspired from stack based decoding for phrase-based
translation~\citep{koehn:2010:book}. We first give an overview
before outlining the algorithm details.

\subsubsection{Overview}
\label{sec:gyroAlgorithmOverview}

In order to represent partial hypotheses, we use a vector
of \emph{columns}. Each column contains a set of
\emph{states}. Each state encodes a set of partial hypotheses.
All states in a column represent hypotheses that cover
the same number of words in the input.

We start to build hypotheses from an initial state that
represents the empty hypothesis. This initial state
is located in the first column, which represents the
hypotheses that have not covered any word in the input.
The empty hypothesis is then extended with the input
$n$-gram rules and their coverage. New states are created
to take into account these extensions. Then, partial
hypotheses represented by these states are iteratively
extended until hypotheses that cover the entire input
are produced.

\subsubsection{Algorithm Details}

As mentioned in \ref{sec:gyroAlgorithmOverview}, we represent
partial hypotheses with a vector of columns, that we denote
$M$. For an input $\bm{w}$ of size $|\bm{w}|$, $M$ has a size
of $|\bm{w}| + 1$. Elements of $M$ are denoted $M_0$, ..., $M_{|\bm{w}|}$.
The $i$-th column $M_i$ represents hypotheses that have covered
$i$ words in the input. The columns $M_i$ are the analog of stacks
in stack based decoding for phrase-based translation.

Each column contains a set of states that encode a set of partial
hypothesis. A state contains the information necessary to
extend a partial hypothesis and represents a set of
partial hypotheses that can be \emph{recombined}~\citep{koehn:2010:book}.
This means that if two partial hypotheses $h_1$ and $h_2$
are represented by a state $s$ and $h_1$ has a higher model score than
$h_2$, then if $h_1$ and $h_2$ are extended with the same rules, then
the extension of $h_1$ will have a higher score than the extension $h_2$.

The states are sorted by their \emph{score}.
For a given state, its score is the best model score of any of the partial
hypotheses that the state represents.

The decoding algorithm is presented in \autoref{alg:gyroDecoder}.
%
\begin{figure}
  \begin{algorithmic}[1]
    \Function{Decode}{$\bm{w}, \mathcal{N}, \mathcal{G}$}
      \State{\Call{Initialize}{$M$}} \hypertarget{alg:line:initM}{} \label{alg:line:initM}
      \For{$0 \leq i < |\bm{w}|$} \hypertarget{alg:line:loopCoverage}{} \label{alg:line:loopCoverage}
        \For{$\text{state } s \in M_i$} \hypertarget{alg:line:loopState}{} \label{alg:line:loopState}
          \For{$\text{ngram } r \in \mathcal{N}$} \hypertarget{alg:line:loopNgram}{} \label{alg:line:loopNgram}
            \If{\Call{CanApply}{$s, r$}} \hypertarget{alg:line:canApply}{} \label{alg:line:canApply}
              \State{\Call{Extend}{$s, r$}} \hypertarget{alg:line:extend}{} \label{alg:line:extend}
            \EndIf
          \EndFor
        \EndFor
      \EndFor
    \EndFunction
  \end{algorithmic}
  \caption{NgramGen decoding algorithm. The input is a bag of words $\bm{w}$,
  a set $\mathcal{N}$ of $n$-gram rules with their coverage and a language model
  $\mathcal{G}$. The column vector $M$ is first initialized with a state representing
  the empty hypothesis. Then, the initial state is iteratively extended.}
  \label{alg:gyroDecoder}
\end{figure}
%
We first initialize the matrix $M$ to be a vector of empty
columns of size $|\bm{w}| + 1$ (\hyperlink{alg:line:initM}{line \ref{alg:line:initM}}).
The first column is filled with an initial state representing an empty hypothesis.
Then, we loop over the column indices
(\hyperlink{alg:line:initM}{line \ref{alg:line:loopCoverage}}), the states in each
column (\hyperlink{alg:line:loopState}{line \ref{alg:line:loopState}}) and the
$n$-gram rules (\hyperlink{alg:line:loopNgram}{line \ref{alg:line:loopNgram}}).
In each case, we attempt to extend a state with an $n$-gram rule. We first test
if the $n$-gram rule $r$ is applicable to state $s$
(\hyperlink{alg:line:canApply}{line \ref{alg:line:canApply}}). If this
is the case, we extend the state $s$ with the rule $r$
(\hyperlink{alg:line:extend}{line \ref{alg:line:extend}}).

We will now describe the last two operations, \textsc{CanApply}
and \textsc{Extend} but before that, we need to describe what kind
of information a state contains.

\paragraph{State Definition} The states
contain enough information
in order to extend a hypothesis and represent
all hypotheses that can be recombined. A state therefore
contains the following information:
%
\begin{itemize}
  \item Coverage: the coverage is a bit vector that indicates
    which words in the input have been covered. This implies a
    sorting of the input bag of words. The sorting is arbitrary
    and we simply choose to represent the bag of words by the
    sequence of words to be recovered.
  \item History: in order to compute the $n$-gram language model
    score correctly when extending a partial hypothesis, we
    store the last $n-1$ words of the partial hypothesis we want
    to extend. The definition of history is therefore the same
    as the definition of history for an $n$-gram language model.
\end{itemize}

\paragraph{\textsc{CanApply} Operation}

Given a state $s$ with coverage $c(s)$ and an $n$-gram rule $r$ with
coverage $c(r)$, rule $r$ can be applied to state $s$ iff $c(s)$ and
$c(r)$ are disjoint. We will see in \autoref{sec:overlap}
that this constrain can be relaxed if we allow for overlapping
$n$-grams.

\paragraph{\textsc{Extend} Operation}

Given a state $s$ with coverage $c(s)$ and and $n$-gram rule $r$
with coverage $c(r)$ that can be applied to $s$, we extend
$s$ with $r$ into a new state $s'$ as follows:
%
\begin{itemize}
  \item The coverage $c(s')$ of $s'$ is the bitwise OR
    of $c(s)$ and $c(r)$:
%
\begin{equation}
  c(s') = c(s) \mid c(r)
\end{equation}
%
  \item The new partial score $\text{sc}(s')$ for $s'$ is defined in terms
    of the partial score $\text{sc}(s)$ for $s$, the history $h(s)$ and the rule $r$:
%
\begin{equation}
  \text{sc}(s') = \text{sc}(s) + \lambda_g g(r | h(s)) + \bm{\lambda_{\varphi}} \cdot \bm{\varphi(r)}
\end{equation}
%
  \item If $s'$ already exist in the column corresponding to its coverage, then
    the score of the existing state is updated if $\text{sc}(s')$ is better than
    the existing score. Otherwise, $s'$ is simply added to the column corresponding
    to its coverage.
\end{itemize}
%

\paragraph{Pruning}
Our algorithm does not solve the issue that the search space for an input
of size $N$ with distinct words has a size $N!$. We therefore need pruning
in search.
We support histogram pruning and threshold pruning~\citep{koehn:2010:book}.
After each extension, if we add a new state, we enforce that the column where
a state was added satisfies the pruning constraints. This means that for
histogram pruning with $m$ the maximum number of states per column, if we
add a new state to a column that has $m$ states, then the state with the lowest
score is removed from that column.

For threshold pruning with a threshold $t$, the situation is
slightly more complex. When we want to add a state with score $s$ to a column where the best score
is $b$, we consider three cases:
%
\begin{itemize}
  \item $s \leq b$ and $s \geq b - t$: the state is added to the column.
  \item $s \leq b$ and $s < b - t$: the state is not added to the column.
  \item $s > b$: the state is added to the column and all states in that
    column with a score less than $s - t$ are removed from the column.
\end{itemize}

\subsection{FST Building}

We mentioned earlier that a state represents partial hypotheses that can
be recombined. However, since we do not keep back pointer between states,
we cannot recover hypotheses directly from the column vector $M$.
Instead, we build an FST that contains all hypotheses. For each extension,
an arc is added to the output FST. We make use of the OpenFst
toolkit~\citep{allauzen-riley-schalkwyk-skut-mohri:2007:CIAA}.
This allows us to rescore the output using tools already integrated
with OpenFst~\citep{blackwood:2010:PHD}.

Another advantage is that hypotheses that are recombined are
not deleted and all hypotheses are stored in the FST. This allows hypotheses
that would be discarded in traditional stack based decoding
to remain and get a chance to get a better score in rescoring.

\subsection{Example}

We now demonstrate how our algorithm works with an example.
Our input consists of:
%
\begin{itemize}
  \item a bag of words: $[\text{do, you, do}]$. Note that the word \emph{do} is repeated.
  \item a bigram language model. Thus the history for each state will consist of the
    last word of the partial hypothesis.
  \item monolingual $n$-gram rules with their coverage of the input bag of word. We
    use the rules listed in \autoref{tab:monolingualRules}.
\end{itemize}
%
\begin{table}
  \begin{center}
  \begin{tabular}{l|l}
    N-gram & Coverage \\
    \hline
    \emph{do} & 100 \\
    \emph{do} & 001 \\
    \emph{you do} & 011 \\
    \emph{you do} & 110 \\
    \emph{do you} & 110 \\
    \emph{do you} & 011 \\
  \end{tabular}
  \caption{Input monolingual n-gram rules with their coverage.}
  \label{tab:monolingualRules}
  \end{center}
\end{table}
%
After running our algorithm without pruning, we obtain the data structure
represented in \autoref{fig:exampleAlgoGyro}.
%
\begin{figure}
  \scriptsize
%  \tikzstyle{State} = [circle, draw, text width = 1.5cm, align = center]
  \tikzstyle{every state}=[text width = 1.5cm, align = center]
  \tikzstyle{line} = [draw, -latex']

  \begin{center}
    \begin{tikzpicture}
      % Place nodes
      \node [state] (init) {000, $<$/s$>$};
      \node [state, above right = 1cm and 1cm of init] (do) {100, do};
      \node [state, right = 2cm of init] (youdo) {011, do};
      \node [state, below right = 1cm and 2.53cm of init] (doyou) {110, you};
      \node [state, right = 2.5cm of do, accepting] (dodoyou) {111, you};
      \node [state, right = 1cm of youdo, accepting] (doyoudo) {111, do};
      % Draw edges
      \path [line] (init) -- node[above, sloped]{do} (do);
      \path [line] (init) -- node[above, sloped]{you do} (youdo);
      \path [line] (init) -- node[above, sloped]{do you} (doyou);
      \path [line] (do) -- node[above, sloped]{do you} (dodoyou);
      \path [line] (do) -- node[above, sloped]{you do} (doyoudo);
      \path [line] (youdo) -- node[above, sloped]{do} (doyoudo);
      \path [line] (doyou) -- node[above, sloped]{do} (doyoudo);
    \end{tikzpicture}
  \end{center}
  \caption{todo}
  \label{fig:exampleAlgoGyro}
\end{figure}
%
We can see that the same state \{111, do\} is reused for
the two hypotheses \emph{do you do}, \emph{you do do}, because
these hypotheses have the same coverage and the same history.
We also note that the hypothesis \emph{do you do} is repeated.
This is not an issue as we use an operation of determinization
on the resulting FST.

\section{Experiments}

In this section, we will present various
string regeneration experiments.  We first present
a baseline without restriction on the input length.
Then, for more rapid experimentation, we conduct
experiments with an input split into chunks of maximum length.
We then show the effect of pruning, $n$-gram rule
length, the use of overlap between rules and finally
the benefits of applying rescoring to our first
pass decoder.

\subsection{Experimental Setup}

We run string regeneration experiments on various
data sets. We use the following data sets:
%
\begin{itemize}
  \item MT08-nw: the first English reference for the newswire portion of the
    Arabic-English translation task for the NIST Open Machine Translation 2008
    Evaluation.\footnote{\url{http://www.itl.nist.gov/iad/mig/tests/mt/2008}}
    This data set is used as a tuning set for MT09-nw.
  \item MT09-nw: the first English reference for the newswire portion of the
    Arabic-English translation task for the NIST Open Machine Translation 2009
    Evaluation.\footnote{\url{http://www.itl.nist.gov/iad/mig/tests/mt/2009}}.
    This data set is used as a testing set and parameters tuned on MT08-nw are used.
  \item WSJ22: Section 22 from the Penn Treebank. This data set is used
    as a tuning set for WSJ23.
  \item WSJ23: Section 23 from the Penn Treebank. This data set is used as a
    testing set and parameters tuned on WSJ22 are used.
\end{itemize}
%
The test sets are first tokenized and lowercased.
Then, $n$-gram rules up to length 5 for each of the processed test sets
are extracted from a large monolingual text collection.
For MT08-nw and MT09-nw, we use all available monolingual data
for the NIST Open Machine Translation 2012
Evaluation.\footnote{\url{http://www.nist.gov/itl/iad/mig/openmt12.cfm}}
This consists of approximately 10.6 billion words.
For WSJ22 and WSJ23, we use all available monolingual data for the WMT13
evaluation.\footnote{\url{http://statmt.org/wmt13/translation-task.html}}.
This consists of approximately 7.8 billion words.
For MT08-nw and MT09-nw, we estimate a modified Kneser-Ney 4-gram language model
on approximately 1.3 billion words of English text, including
the AFP and Xinhua portions of the GigaWord corpus
version 4 and the English side of various Arabic-English
parallel corpora used in MT evaluations. For WSJ22 and WSJ23, we
estimate a modified Kneser-Ney 4-gram language model on all available
monolingual data for the WMT13 evaluation.

\subsection{Baseline}
\label{sec:gyroBaseline}

We first run a baseline on our various test sets with the NgramGen decoder.
Results are reported in \autoref{tab:gyroBaseline}. No restrictions on the
input length are imposed. We use a length dependent histogram pruning of
$\frac{11000}{\text{length(input)}}$. This setting allows the decoding to terminate
while using all the memory available on the machines.
%
\begin{table}
  \begin{center}
  \begin{tabular}{l|l}
    Test Set & BLEU \\
    \hline
    MT08-nw & RUNS \\
    MT09-nw & RUNS \\
    WSJ22 & 44.01 \\
    WSJ23 & 45.48 \\
  \end{tabular}
  \caption{Baseline for the NgramGen Decoder. There are no limits on the input length. Pruning setting are set so that decoding is possible using
  all the memory available on the machines. Performance is measured with case insensitive BLEU.}
  \label{tab:gyroBaseline}
  \end{center}
\end{table}
%
We also compare the performance obtained by the NgramGen decoder with previous
work on the data set WSJ23 in \autoref{tab:gyroComparedPreviousWork}. We can
see that our method, without using any additional information apart from the
input bag-of-word achieves state-of-the-art performance.
%
\begin{table}
\begin{center}
  \begin{tabular}{l|l|l}
    Work & BLEU & Remark \\
    \hline
    Wan et al. EACL'09 & 33.7 & POS tag info + base noun phrase\\
    Zhang and Clark EMNLP'11 & 40.1 & POS tag info \\
    Zhang et al. EACL'12 & 43.8 & POS tag info \\
    Zhang IJCAI'13 & 44.7 & Base phrase info \\
    This work & 45.48 & No additional info
  \end{tabular}
\end{center}
\caption{Comparing the NgramGen decoder performance with previous work
  on the WSJ23 data set.}
\label{tab:gyroComparedPreviousWork}
\end{table}

\subsection{Sentence Splitting}

%TODO remark about the fact that when input is translation output,
%sentence splitting is not cheating
Running our decoder without limiting the length of the input
as in \autoref{sec:gyroBaseline} can be time and memory consuming.
For more rapid experimentation, we only reorder chunks of a limited
size in the
input. The input is divided each time a punctuation sign such
as a comma or a semi-colon is observed or after having seen a maximum
number of tokens. We also use a histogram pruning of 1000 states
regardless of the input length.
With this restriction, we obtain the results reported
in \autoref{tab:gyroBaselineChopping}. We will use this baseline
for more rapid experimentation and further analysis. Specifically,
we will conduct experiments with a relatively short maximum chunk
length of 11 and a more realistic maximum chunk length of 20.

\begin{table}
  \begin{center}
  \begin{tabular}{l|l}
    Maximum Chunk Length & MT08-nw BLEU \\
    \hline
    7 & 82.83 \\
    9 & 78.53 \\
    11 & 74.86 \\
    13 & 71.75 \\
    15 & 68.95 \\
    20 & 64.36 \\
  \end{tabular}
  \caption{NgramGen decoder baseline with restrictions on the input length.}
  \label{tab:gyroBaselineChopping}
  \end{center}
\end{table}

\subsection{Pruning}

In this section, we observe the effect of histogram pruning for
a maximum input length of 11 and 20 in \autoref{tab:gyroPruning}.
In both these settings,
increasing the maximum stack size increases performance, but
more so with maximum input length of 20 since the search space
is much larger in that case.
%
\begin{table}
  \begin{center}
  \begin{tabular}{l|l|l|l}
    Pruning & Max Input Length & MT08-nw BLEU \\
    \hline
    500  & 11 & 74.67 \\
    1000 & 11 & 74.86 \\
    1500 & 11 & 74.98 \\
    2000 & 11 & 75.02 \\
    3000 & 11 & 75.09 \\
    4000 & 11 & 75.08 \\
    5000 & 11 & 75.06 \\
    6000 & 11 & 75.12 \\
    7000 & 11 & 75.15 \\
    8000 & 11 & 75.15 \\
    9000 & 11 & 75.14 \\
    10000 & 11 & 75.14 \\
    \hline
    500  & 20 & 64.21 \\
    1000 & 20 & 64.36 \\
    1500 & 20 & 64.85 \\
    2000 & 20 & 64.94 \\
    3000 & 20 & 65.17 \\
    4000 & 20 & 65.33 \\
    5000 & 20 & 65.39 \\
    6000 & 20 & RUNS \\
    7000 & 20 & RUNS \\
    8000 & 20 & RUNS \\
    9000 & 20 & RUNS \\
    10000 & 20 & RUNS \\
  \end{tabular}
  \caption{Effect of histogram pruning on performance with a maximum input length
  of 11 and 20.}
  \label{tab:gyroPruning}
  \end{center}
\end{table}

\subsection{Impact of Rules}

In this section, we observe the impact of using all rules from unigrams
up to 5-grams or only using rules with specific lengths. Results
are reported in \autoref{tab:gyroVaryNgramLength}.
We can see that for a relatively short maximum of 11 on the input length,
using more $n$-gram orders is always better whereas when we use
a more realistic maximum input length of 20, using more $n$-gram orders
can hurt performance. However, in both cases, it is always beneficial
to not only use unigrams.
%
\begin{table}
  \begin{center}
  \begin{tabular}{l|l|l}
    Rule Configuration & Max Input Length & MT08-nw BLEU \\
    \hline
    All rules & 11 & 74.86 \\
    1g/4g/5g & 11 & 74.85 \\
    1g/5g & 11 & 74.82 \\
    1g only & 11 & 74.13 \\
    \hline
    All rules & 20 & 64.36 \\
    1g/4g/5g & 20 & 64.42 \\
    1g/5g & 20 & 64.74 \\
    1g only & 20 & 63.36 \\
  \end{tabular}
  \caption{Effect of using $n$-gram rules with various length.}
  \label{tab:gyroVaryNgramLength}
  \end{center}
\end{table}

\subsection{Overlap}
\label{sec:overlap}

We saw in \autoref{sec:canApply} that when we extend
a state with a rule, the state coverage and the rule
coverage have to be disjoint. In this section, we relax
this constraint to allow for a coverage intersection
of a maximum length. Results are presented in
\autoref{tab:overlap}. Overall, we can see
that allowing for overlap has no impact on performance.
We do not use this decoder capability in future experiments.
% TODO look at effect when no unigrams are used

\begin{table}
  \begin{center}
  \begin{tabular}{l|l|l|l}
    Overlap & Max Chop &  Configurations & MT08-nw BLEU \\
    \hline
    No overlap & 11 & All & 74.86 \\
    1          & 11 & All & 74.87 \\
    2          & 11 & All & 74.85 \\
    \hline
    No overlap & 11 & 1g/5g & 74.82 \\
    1          & 11 & 1g/5g & 74.81 \\
    2          & 11 & 1g/5g & 74.80 \\
    \hline
    No overlap & 20 & All & 64.36 \\
    1          & 20 & All & 64.39 \\
    2          & 20 & All & 64.28 \\
    \hline
    No overlap & 20 & 1g/5g & 64.74 \\
    1          & 20 & 1g/5g & 64.72 \\
    2          & 20 & 1g/5g & 64.63 \\
  \end{tabular}
  \caption{Effect of allowing for overlapping rules.}
  \label{tab:overlap}
  \end{center}
\end{table}

\subsection{Rescoring}

Because our decoder outputs a lattice of hypotheses, this
allows us to apply various lattice rescoring
techniques~\citep{blackwood:2010:PHD}.
We demonstrate in \autoref{tab:gyroRescoring}
that those rescoring techniques are also effective in the
string regeneration task setting.
%
\begin{table}
  \begin{center}
  \begin{tabular}{l|l|l|l|l}
    Config & MT08-nw & MT09-nw & WSJ22 & WSJ23 \\
    \hline
    1st Pass & 64.36 & 60.91 & 62.76 & 64.39 \\
    5g       & 66.11 & 63.26 & 63.18 & 65.04 \\
    LMBR     & 67.37 & 64.53 & 64.28 & 65.85 \\
  \end{tabular}
  \caption{Effect of lattice rescoring on the first pass lattices obtained
  by the NgramGen decoder.}
  \label{tab:gyroRescoring}
  \end{center}
\end{table}

\subsection{Future Cost}
\label{sec:gytoFutureCost}

Hypotheses that cover the same number of words in the input
bag of words are grouped together in a stack and
hypotheses with costs that are too high are pruned out.
However, these hypotheses may not be directly comparable, because
some hypotheses cover very frequent words that are favored by the
language model while other hypotheses may cover infrequent words.

Thus we estimate a future cost for each hypothesis and that future
cost is added to the hypothesis cost for pruning. Because there
is no notion of word ordering in the input, we use a unigram
language model to estimate future cost.

\autoref{tab:futureCost} shows that adding a unigram language model
future cost estimate to the cost for pruning is beneficial.
For a short maximum length of 11 on the input length, we
obtain a gain of 0.24 BLEU while for a maximum input length
of 20, we obtain a gain of 0.69 BLEU.
%
\begin{table}
  \begin{center}
    \begin{tabular}{lll}
      Configuration & Max Chop & MT08-nw BLEU \\
      No Future Cost & 11 & 74.86 \\
      Future Cost & 11 & 75.10 \\
      No Future Cost & 20 & 64.36 \\
      Future Cost & 20 & 65.05 \\
    \end{tabular}
    \caption{Effect of using a future cost estimate for pruning.
    Future cost is estimated with a unigram language model.
    Future cost is beneficial for a relatively short maximum input
    length of 11 and for a more realistic maximum input length of 20.}
    \label{tab:futureCost}
  \end{center}
\end{table}

\section{Conclusion}

In this chapter, we have presented a novel approach to the string regeneration
task, inspired from phrase-based SMT. Our decoder NgramGen achieves state-of-the-art
performance on string regeneration from a simple bag of word on the Penn Treebank
Section 23 data set.

In this following chapter, we will explore potential application of our decoder
to word reordering in machine translation.

%TODO dep lm
% TODO      \item analysis of when the input is lost
% TODO for translation column about translation results + oracle for 1-best or 10-best. show get same score with combination or biased lm.

%There is a separate report on all experiments (attached).
%Here are possible experiments:
%
%\begin{itemize}
%  \item investigate various pruning thresholds and see impact on BLEU. Show oracle scores as well. Maybe compare to the FST-based Gyro software.
%  \item investigate how useful are longer n-gram rules. For example, see the impact of using only unigram rules.
%  \item investigate if it is useful to use overlap in decoding (overlap means that we can cover a input position more than once).
%  \item show gains from rescoring: 5g rescoring, LMBR rescoring and combination with MT hypothesis space.
%\end{itemize}

%\section{String Regeneration Experiments with Dependency LM}

%\begin{itemize}
%  \item Rescoring with dependency LM: gyro generates and n-best list which can be rescored with dependency LMs.
%  \item What happens if we only rescore with left LM, right LM or head LM.
%  \item Compare rescoring with dependency LM directly integrated into Gyro.
%\end{itemize}
