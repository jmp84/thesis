\chapter{Introduction}
\label{chap:intro}

\section{Machine translation}

% This section should define what machine translation is.
% It should also describe what level of quality it can
% currently achieve. Mention the term ``gist''.

% TODOFINAL harmonize references to NIST openmt
% TODOFINAL check abbrev for CUED

Machine translation is the process of translation from input
speech or text in a natural language into another natural language
by some kind of automatic system.
Real world examples include online services such as
Google Translate\footnote{\url{https://translate.google.com}},
Bing Translate\footnote{\url{https://www.bing.com/translator}},
SDL\footnote{\url{http://www.freetranslation.com/}},
PROMT\footnote{\url{http://www.online-translator.com}}, etc.
Interacting with any online automatic translation service with
the expectation of a high quality translation can be a frustrating
experience. Indeed, variations
in word order across languages and syntax and the use of real world knowledge
for puns or idioms make translation a very challenging task.

\subsection{Challenges for Translation}

In order to illustrate the difficulties that arise in
translation, we present several examples that
make translation challenging for humans, and \emph{a fortiori} for
computers. In some languages, some concepts are common enough
to be designated by one word, but in another language,
an entire sentence may be needed to describe that concept. For example, the
word \emph{sobremesa} in Spanish can be translated into English
as \emph{the time spent after a meal, talking to the people with whom the meal was shared}.\footnote{\url{http://blog.maptia.com/posts/untranslatable-words-from-other-cultures}}
In this situation, a human translator is left with the choice of
keeping the translation short but inexact or long but cumbersome.
For a computer, or rather a statistical model, such a situation will represent
an outlier in terms of the ratio of the number of words that are translation of each other.
Self-referential sentences can also present challenges for translation.
For example, the sentence \emph{This sentence has five words}
has at least two acceptable translation into Russian from the syntactic and
semantic point of view: \emph{В этом предложении пять слов} and
\emph{Это предложение состоит из пяти слов}. However, the second translation
has six words and therefore cannot be accepted.
Another challenge for translation is word ordering. The first sentence
of the novel \emph{The Metamorphosis} by Franz Kafka reads % TODOFINAL use quote env
\emph{Als Gregor Samsa eines Morgens aus unruhigen Träumen erwachte, fand er sich in seinem Bett zu einem ungeheuren Ungeziefer verwandelt}.
One possible English translation is \emph{As Gregor Samsa awoke one morning from uneasy dreams, he found himself transformed in his bed into a gigantic insect-like creature}.
In German, the words for \emph{transformed} (\emph{verwandelt}) and \emph{insect} (\emph{Ungeziefer}) come right at the end
of the sentence and create an effect of surprise for the reader. In English, however, the verb \emph{transformed}
comes in the middle of the sentence and the effect of surprise is not
as great. This example demonstrates how variations in word ordering between languages can
make translation challenging. Computers have the additional challenge that the choice
of word ordering should produce a grammatical sentence. This is a challenge for humans too
but computers are particularly bad at producing a grammatical output.

\subsection{Machine Translation Current Quality}

Even though machine translation is a challenging task, it is
still useful in a number of situations. For example, machine
translation can be used to obtain the \emph{gist}, i.e. the general
meaning, of a document
in a foreign language. Machine translation can also be
used for post-editing: a document in a foreign language
is first automatically translated and then the translation
is corrected by a human translators. This is precisely
the approach taken by translation companies such
as Unbabel\footnote{\url{https://www.unbabel.com}}.
% TODONEVER expand the number of useful applications

Machine translation has therefore gotten to the point
where it is actually useful for practical applications, but
it is reasonable to ask how far we are from perfect translation.
We give indications in terms of the BLEU
metric~\citep{papineni-roukos-ward-zhu:2002:ACL},
which we will define in \autoref{sec:evaluationMetrics}.
For now, we simply need to know that BLEU measures how well
a translation hypothesis resembles a set of human translation
references. The Cambridge University Engineering Department (CUED)
submitted a translation system to the NIST Open Machine Translation 2012
Evaluation.\footnote{\url{http://www.nist.gov/itl/iad/mig/openmt12.cfm}}
We run this system on the newswire portion of the
NIST Open Machine Translation 2008 Evaluation test set (MT08).
For this MT08 test set, 4 references are available. We measure
the case insensitive BLEU score of the CUED system against
the 4 references and against subsets of 3 references. Similarly,
we measure the case insensitive BLEU score of each reference
against the other references. BLEU scores for humans and for
the CUED system are summarised in \autoref{tab:humanAndSystemBLEU}.
On average, CUED obtains a
BLEU score of 35.39 against 3 references. On average, human
references obtain a BLEU score 46.45 against the other
references.
%\autoref{fig:goodQualitySentences}.
%
\begin{table}
  \begin{center}
    \begin{tabular}{l|lllll}
      System & 1-2-3-4 & 2-3-4 & 1-3-4 & 1-2-4 & 1-2-3 \\
      \hline
      CUED   & 38.95 & 34.80 & 35.60 & 35.87 & 35.28 \\
      \hline
      Reference 1 & -- & 46.19 & -- & -- & -- \\
      Reference 2 & -- & -- & 47.27 & -- & -- \\
      Reference 3 & -- & -- & -- & 45.43 & -- \\
      Reference 4 & -- & -- & -- & -- & 46.90
    \end{tabular}
    \caption{BLEU score obtained by the Cambridge University Engineering Department
    system and by human translators on the MT08 test set. 4 references are
    provided. The BLEU score is measured against various subset of references: all
    references (1-2-3-4), all but the first (2-3-4), etc. The BLEU score for
    a human reference when the set of reference contains the human reference is
    not computed and is simply 100.}
    \label{tab:humanAndSystemBLEU}
  \end{center}
\end{table}
%
We can draw two conclusions from these observations.
First, this is evidence that many possible translations are
acceptable and this is why even humans cannot obtain a BLEU
score close to 100\%. Second, because the automatic
system performance is only approximately 10 BLEU points below
human performance, we can conclude that the quality of
automatic translation is very high. Manual inspection of the
system output confirms this. We show a few examples of high
quality output sentences that are relatively long in
\autoref{fig:goodQualityOutput}:
%
\begin{figure}
\begin{quote}
      minister of agriculture and social affairs, told afp: ``the prime minister submitted his resignation to abbas, chairman of the president, and at the same time asked him to form a new cabinet, responsible for handling daily affairs of the new government.''

      chavez has ordered government officials to keep an eye on foreigners visiting venezuela's speech, when a person is found to have publicly criticized him or the venezuelan government, are to be deported.

      us-china strategic economic dialogue ``focused on the economic, environmental and other issues, the most important thing is that the issue of the rmb exchange rate, the us congress members are of the view that the value of the renminbi underestimated.
\end{quote}
\caption{Example of output of a translation system. Even though the sentences are relatively long,
they are overall fluent and intelligible.}
\label{fig:goodQualityOutput}
\end{figure}

\section{Statistical Machine Translation}

The current dominant approach to machine translation
is a statistical approach. Given an input in a natural
language, a statistical model of translation will attempt
to predict the best translation according to a statistical
criterion, such as the most likely translation under a
probabilistic model of translation. The data
used to estimate the parameters for such a model consists
of a \emph{parallel corpus}, which is a set of sentence
pairs in two different natural language that are
translation of each other, and a monolingual corpus
which is a set of sentences in the language we would like
to translate into.

Parallel data can be obtained from multilingual institutions
proceedings such as the United
Nations~\citep{franz-kumar-brants:2013:LDC},% TODOFINAL (why ?) fix this citation
the Canadian
Hansard~\citep{germann:2001:WEB} or the
European Parliament~\citep{koehn:2005:MTSummit}.
This type of data is relatively clean since it is created
by professional translators. However, it may not match
the genre of the input sentences we wish to translate, such
as newswire. The importance of having a good match between
the data used for training and testing is discussed
in \autoref{sec:domainAdaptationMT}. In order to address
this concern and also in order to obtain more data, parallel
data can also be extracted automatically from \emph{comparable}
corpora~\citep{smith-saintamand-plamada-koehn-callisonburch-lopez:2013:ACL2013}.
However, the widespread availability of
machine translation and the development of automatic techniques
to extract parallel corpora automatically increase the
risk of having automatically translated output of poor
quality present in the training data. These concerns have
been acknowledged and addressed by a watermarking
algorithm~\citep{venugopal-uszkoreit-talbot-och-ganitkevitch:2011:EMNLP}.

\subsection{The SMT Pipeline}
\label{sec:theSMTpipeline}

Typically, state-of-the-art SMT systems are organised into
a pipeline of deterministic or statistical modules, as shown
in \autoref{fig:mtPipeline}.
Parallel text is first preprocessed. This consists of data cleaning
and tokenisation.
For morphologically poor languages such as
English, simple rules for tokenisation, e.g. separate words by white space
and punctuation, are usually good enough.
For morphologically rich languages, more advanced techniques
are needed for effective tokenisation, for example
morphological analysis~\citep{habash-rambow:2005:ACL}, in order
to combat data sparsity and work with a vocabulary with reasonable size.
In the case of languages without word
boundaries, such as Chinese, word segmentation techniques
need to be applied~\citep{zhang-clark:2007:ACL}, in order
to be able to break sentences into words.

The preprocessed parallel text is then word-aligned
(see \autoref{sec:StatisticalMachineTranslationWordAlignment}).
A word alignment is simply a mapping between source words and
target words in a parallel sentence pair. Word alignment models
were originally used to describe the translation process and
to perform
translation~\citep{germann-jahr-knight-marcu-yamada:2001:ACL}.
Word alignment toolkits are available
online\footnote{\url{http://mi.eng.cam.ac.uk/~wjb31/distrib/mttkv1}}%
      \textsuperscript{,}%%hack because footmisc won't work with footnotebackref i think
      \footnote{\url{https://code.google.com/p/giza-pp}}
as well as a word to word translation
decoder.\footnote{\url{http://www.isi.edu/licensed-sw/rewrite-decoder}}
Nowadays, word alignment models are used as an intermediate
step in the translation pipeline.
A translation grammar is then extracted and estimated
from the word alignments (see \autoref{sec:hierruleextract}) and
translation is performed under the translation grammar.

The monolingual data is also preprocessed in the same fashion as the
target side of the parallel text and one or more language models are
estimated from this data (see section \autoref{sec:languageModelling}).
The language models and the translation grammar are used by
the translation decoder for translation. In order to optimise
the SMT model parameters, alternating decoding and tuning steps
are carried out.

Typically, a decoder can output a best possible
translation, or an $n$-best list of translations or even a lattice
of translations, which is a compact representation of an $n$-best list
with a very large number $n$. In the latter case, optional rescoring steps
can be carried out in order to include models that are too complex
or not robust enough to be included in first pass
decoding (see \autoref{sec:rescoring}). In particular, we study
a model of string regeneration that allows any reordering of the
output of the first pass decoder in \autoref{chap:gyroTrans}.
%
\begin{figure}
  \tikzstyle{TranslationModule} = [rectangle, draw, rounded corners,
    align = center, text width = 4cm]
  \tikzstyle{line} = [draw, very thick, color=black!50, -latex']

  \begin{center}
    \begin{tikzpicture}[node distance = 1.5cm]
      % Place nodes
      \node [TranslationModule] (parallelData) {Parallel Text};
      \node [TranslationModule, below of=parallelData] (preprocessing) {Preprocessing};
      \node [TranslationModule, below of=preprocessing] (wordalignment) {Word Alignment};
      \node [TranslationModule, below of=wordalignment] (rulextraction) {Grammar Extraction};
      \node [TranslationModule, right = 3cm of parallelData] (monolingualData) {Monolingual Text};
      \node [TranslationModule, below = 1.57cm of monolingualData] (preprocessing2) {Preprocessing};
      \node [TranslationModule, below = 1.57cm of preprocessing2] (languageModel) {Language Modelling};
      \node (emptyMiddleGrammarLm) at ($(rulextraction)!0.5!(languageModel)$) {};
      \node [TranslationModule, below of = emptyMiddleGrammarLm] (decoding) {Decoding};
      \node [TranslationModule, below of=decoding] (tuning) {Tuning};
      \node [TranslationModule, below of=tuning] (rescoring) {Rescoring};
      % Draw edges
      \path [line] (parallelData) -- (preprocessing);
      \path [line] (preprocessing) -- (wordalignment);
      \path [line] (wordalignment) -- (rulextraction);
      \path [line] (monolingualData) -- (preprocessing2);
      \path [line] (preprocessing2) -- (languageModel);
      \path [line] (rulextraction) -- (decoding);
      \path [line] (languageModel) -- (decoding);
      \path [line] (decoding) edge [bend right] (tuning);
      \path [line] (tuning) edge [bend right] (decoding);
      \path [line] (tuning) -- (rescoring);
      % Circle important modules
      %\node [ellipse, draw=red, fit= (wordalignment)] {};
      %\node [ellipse, draw=red, fit= (rulextraction)] {};
    \end{tikzpicture}
    \caption{Machine Translation System Development Pipeline}
    \label{fig:mtPipeline}
  \end{center}
\end{figure}

\section{Contributions}

The contributions of this thesis are outlined as follows:
%
\begin{itemize}
  \item We describe a novel approach to grammar extraction
    and estimation. Our approach ties the models of word
    alignment and grammar extraction and estimation more
    closely. It results in a more robust extraction and estimation
    of translation grammars and leads to improvements in
    translation quality, as demonstrated in Chinese to English
    translation experiments.
  \item We describe a system that
    allows the efficient extraction
    and filtering of very large grammars.
    This method has been in continued use at
    CUED and was employed for submissions to
    the NIST 2012\footnote{\url{http://www.nist.gov/itl/iad/mig/openmt12.cfm}}
    and the WMT
    2013~\citep{bojar-buck-callisonburch-federmann-haddow-koehn-monz-post-soricut-specia:2013:WMT} translation
    evaluations. This was a carefully engineering effort
    that required detailed understanding of grammar
    extraction procedures and how to implement them in a
    Hadoop framework. There are many implementation
    strategies that can be taken, but obtaining the processing
    performance needed to support the experiments
    reported in this thesis requires a careful design process.
  \item We designed and implemented a system
    for string regeneration, inspired from
    phrase-based SMT techniques. We obtain
    state-of-the-art results in the string
    regeneration task and demonstrate potential
    applications to machine translation.
\end{itemize}

\section{Organisation of the thesis}

We now describe the thesis organisation.
In \autoref{chap:smt}, we review statistical
machine translation background: all components
of the machine translation pipeline presented in \autoref{fig:mtPipeline}
are reviewed in detail.
In \autoref{chap:hfile}, we present our system for efficient
extraction of translation grammars from parallel
text and retrieval of translation rules
from very large translation grammars.
In \autoref{chap:extractionFromPosteriors}, we
present our novel grammar extraction procedure that makes use of
posterior probabilities from word alignment models
We then provide hierarchical phrase-based system building recommendations
about what decisions to make in terms of language modelling
and grammar design to obtain the best possible systems
for translation in \autoref{chap:wmt}.
In \autoref{chap:gyro}, we introduce our
phrase-based decoder for string regeneration and study
fluency through the word reordering task.
Finally, in \autoref{chap:gyroTrans}, we apply our
regeneration decoder to the output of a machine translation system.
